# -*- coding: utf-8 -*-
"""posNegWord.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17QWvx5B4wR1KYBCM8DPb333ROibhjKkb
"""

import pandas as pd

df = pd.read_csv('에타리뷰_수정.csv')

df.shape

df.isnull().sum()

df.head(30)

df['평점'].value_counts()

긍정 = df[df['평점']>3]

부정 = df[df['평점'] < 3]

result = pd.concat([긍정,부정])

result

def rating_to_label(rating):
    if rating > 3:
        return 1 #긍정
    else:
        return 0 #부정
    
result['y'] = result['평점'].apply(lambda x: rating_to_label(x))

result

"""# 전처리"""

import re

def apply_regular_expression(text):
    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글
    result = hangul.sub('', text)  # 위에 설정한 "hangul"규칙을 "text"에 적용(.sub)시킴
    return result

test = df['리뷰'][2]

test

apply_regular_expression(test)

# 명사 형용사 추출하기 (Mecab)

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# apt-get update
# apt-get install g++ openjdk-8-jdk python-dev python3-dev
# pip3 install JPype1
# pip3 install konlpy

# Commented out IPython magic to ensure Python compatibility.
# %env JAVA_HOME "/usr/lib/jvm/java-8-openjdk-amd64"

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)
# pip3 install /tmp/mecab-python-0.996

import konlpy
from konlpy.tag import Kkma, Komoran, Hannanum, Okt
from konlpy.utils import pprint
from konlpy.tag import Mecab

def tokenizing(data):
    mecab = Mecab()
    stopwords = ['강의' '건대', '교수', '대학', '사람', '설명', '수업', '심교', '시대', '영화', '학생', '교양','시험']
    
    def extract_noun(sent):
        sent = re.sub(r'[^\s\da-zA-Z가-힣]', ' ', sent) # 숫자, 공백, 문자 외 제거
        sent = re.sub(r'\s{2,}', '\n', sent).strip() # 공백이 2개 이상 반복되는 경우 하나만 남김
        # s = mecab.nouns()
        nouns = mecab.pos(sent)
        # nouns = [w[0] for w in s if (w[1] == 'Noun' and len(w[0]) > 1)] # 두 글자 이상인 명사만 추출
        return nouns
    
    #tokens = [[word for word,tag in (extract_noun(sent)) if not word in stopwords and len(word) > 1] for sent in data]
    tokens = [[word for word,tag in (extract_noun(sent)) if not word in stopwords and len(word) > 1 and tag in ["NNG","NNP","VA","VA+ETM"]] for sent in data]
    # 명사 "NNG","NNP"
    # 동사 "vv"
    # 형용사 "va" 

    return tokens

# 코드 테스트
test = df['리뷰'][2]
apply_regular_expression(test)

# 코드 테스트
from konlpy.tag import Mecab
mecab = Mecab()

nouns = [word for word, tag in mecab.pos(apply_regular_expression(test)) if tag in ["NNG","NNP","VA","VA+ETM"]]
#nouns = [[word for word,tag in mecab.pos(apply_regular_expression(test)) if tag in ["NNG","NNP","VA","VA+ETM"]]
#nouns = [word for word, tag in mecab.pos(apply_regular_expression(test)) if ]

nouns

# 전체 리뷰 말뭉치 생성
corpus = "".join(result['리뷰'].tolist())
#corpus

apply_regular_expression(corpus)

# 전체 말뭉치(corpus)에서 명사, 형용사 형태소 추출
nouns = [word for word, tag in mecab.pos(apply_regular_expression(corpus)) if tag in ["NNG","NNP","VA","VA+ETM"]]
# nouns = okt.nouns(apply_regular_expression(corpus))
print(nouns)

from collections import Counter

# 빈도 탐색
counter = Counter(nouns)

# 빈도 높은 상위 10개
counter.most_common(30)

available_counter = Counter({x: counter[x] for x in counter if len(x) > 1})
available_counter.most_common(30)

# 불용어사전
stopwords = pd.read_csv("https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt").values.tolist()
stopwords[:10]

강의평가_stopwords = ['강의' '건대', '교수', '대학', '사람', '설명', '수업', '심교', '시대', '영화', '학생', '교양','시험']
for word in 강의평가_stopwords:
    stopwords.append(word)

"""# Word Count

Bow 벡터 생성
"""

from sklearn.feature_extraction.text import CountVectorizer

def text_cleaning(text):
    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리
    result = hangul.sub('', text)
    mecab = Mecab()  # 형태소 추출
    nouns = mecab.pos(result)
    nouns = [word for word, tag in nouns if tag in ["NNG","NNP","VA","VA+ETM"] and word not in stopwords] 

    # 불용어 제거 + 명사, 형용사만 추출
    #nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거
    #nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거
    return nouns

vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))
bow_vect = vect.fit_transform(result['리뷰'].tolist())
word_list = vect.get_feature_names() #단어리스트
count_list = bow_vect.toarray().sum(axis=0)

word_list

# 각 단어가 전체 리뷰중에 등장한 총 횟수
count_list

# 각 단어의 리뷰별 등장 횟수
bow_vect.toarray()

bow_vect.shape

# "단어" - "총 등장 횟수" Matching

word_count_dict = dict(zip(word_list, count_list))
word_count_dict

"""# TF-IDF 적용"""

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_vectorizer = TfidfTransformer()
tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)

print(tf_idf_vect.shape)

# 첫 번째 리뷰에서의 단어 중요도(TF-IDF 값) -- 0이 아닌 것만 출력
print(tf_idf_vect[0])

# 첫 번째 리뷰에서 모든 단어의 중요도 -- 0인 값까지 포함
print(tf_idf_vect[0].toarray().shape)
print(tf_idf_vect[0].toarray())

vect.vocabulary_

invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}
print(str(invert_index_vectorizer)[:100]+'...')

"""# 감성분류  : Logistic Regression

전처리된 리뷰 데이터를 이용하여 감성분류 예측 모델 만들기
"""

result.sample(10)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

result['평점'].hist()
# 4~5 : 긍정 > 1, 1~3 : 부정 > 0

def rating_to_label(rating):
    if rating > 3:
        return 1
    else:
        return 0
    
result['y'] = result['평점'].apply(lambda x: rating_to_label(x))

result

result["y"].value_counts()

"""# train set / test set 나누기"""

from sklearn.model_selection import train_test_split

x = tf_idf_vect
y = result['y']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)

x_train.shape, y_train.shape

x_test.shape, y_test.shape

"""# 모델학습"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# fit in training set
lr = LogisticRegression(random_state = 0)
lr.fit(x_train, y_train)

# predict in test set
y_pred = lr.predict(x_test)

# 분류 결과

print('accuracy: %.2f' % accuracy_score(y_test, y_pred))
print('precision: %.2f' % precision_score(y_test, y_pred))
print('recall: %.2f' % recall_score(y_test, y_pred))
print('F1: %.2f' % f1_score(y_test, y_pred))

# confusion matrix

from sklearn.metrics import confusion_matrix

confu = confusion_matrix(y_true = y_test, y_pred = y_pred)

plt.figure(figsize=(4, 3))
sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')
plt.title('Confusion Matrix')
plt.show()

"""## 클래스 불균형 조정하기"""

result=result.reset_index(drop=True)

positive_random_idx = result[result['y']==1].sample(275, random_state=12).index.tolist()
negative_random_idx = result[result['y']==0].sample(275, random_state=12).index.tolist()

random_idx = positive_random_idx + negative_random_idx

random_idx = positive_random_idx + negative_random_idx
x = tf_idf_vect[random_idx]
y = result['y'][random_idx]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)

x_train.shape, y_train.shape

x_test.shape, y_test.shape

"""# 모델 재학습"""

lr2 = LogisticRegression(random_state = 0)
lr2.fit(x_train, y_train)
y_pred = lr2.predict(x_test)

# 분류 결과 평가

print('accuracy: %.2f' % accuracy_score(y_test, y_pred))
print('precision: %.2f' % precision_score(y_test, y_pred))
print('recall: %.2f' % recall_score(y_test, y_pred))
print('F1: %.2f' % f1_score(y_test, y_pred))

# confusion matrix

from sklearn.metrics import confusion_matrix

confu = confusion_matrix(y_true = y_test, y_pred = y_pred)

plt.figure(figsize=(4, 3))
sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')
plt.title('Confusion Matrix')
plt.show()

"""# 긍부정 키워드 분석하기"""

# print logistic regression's coef

plt.figure(figsize=(10, 8))
plt.bar(range(len(lr2.coef_[0])), lr2.coef_[0])

"""계수가 양수인 경우는 긍정적인 단어, 음수인 경우는 부정적인 단어"""

# 긍정/부정 top 5 출력

print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[:5])
print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[-5:])

coef_pos_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)
coef_neg_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = False)
coef_pos_index

invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}
invert_index_vectorizer

pos_list = []
for coef in coef_pos_index[:100]:
    print(invert_index_vectorizer[coef[1]], coef[0])
    pos_list.append(invert_index_vectorizer[coef[1]])
# 긍정

# 긍정단어로 보기 어려운 단어들 제거
not_pos = ['힘들','크','공직','큰','북한','경영학','보고서','대면','북','녹음','같','글','립니','판례','어려운','이과','숨통','걱정','강의','이름','상당','말씀','건강','하루','정직','아침','요구','교재','짜임','실험','정리','어렵','시국','계절','토론','당','작성','감점','폭탄','고학년','법','뇌','문과','행정','경제학','생각','감상']
for x in not_pos:
  pos_list.remove(x)

pos_list

neg_list = []
for coef in coef_neg_index[:100]:
    print(invert_index_vectorizer[coef[1]], coef[0])
    neg_list.append(invert_index_vectorizer[coef[1]])

# 부정

neg_list
# 부정단어로 보기 어려운 단어들 제거
not_neg = ['러닝','기준','시간','채점','답','성적','셈','학점','재미','용','뒤','거리','인','ㄹ','속','올해','현','해','자리','작품','중심','의문','복','기분','수월','현장']

for x in not_neg:
  neg_list.remove(x)

neg_list

neg_list